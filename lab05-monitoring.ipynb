{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "\n",
    "Azure AI Agent Service supports tracing where all requests can be sent to Application Insights and can be used to monitor agents.\n",
    "\n",
    "This notebook shows how to do it, but Application Insights was not provisioned automatically. If you want to follow this notebook you need to deploy it by yourself. You can find more information on how to do it [here](https://learn.microsoft.com/en-us/azure/ai-services/agents/concepts/tracing).\n",
    "\n",
    "This tracing capability requires some Python libraries which the code below will install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opentelemetry\n",
    "%pip install azure-core-tracing-opentelemetry\n",
    "%pip install azure-monitor-opentelemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some Python classes, functions and definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from pprint import pp as pp\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from opentelemetry import trace\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.ai.projects.models import FunctionTool, RequiredFunctionToolCall, SubmitToolOutputsAction, ToolOutput\n",
    "from typing import Any, Callable, Set\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def pprint(obj):\n",
    "    pp(obj.as_dict() if hasattr(obj, \"as_dict\") else obj, width=100)\n",
    "\n",
    "# Print the environments we will be using.\n",
    "print(f\"PROJECT_CONNECTION_STRING: {os.getenv('PROJECT_CONNECTION_STRING')}\")\n",
    "print(f\"BING_CONNECTION_NAME: {os.getenv('BING_CONNECTION_NAME')}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the project reference using the current authenticated user and the connection string to the project where the agents will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Application Insights connection string linked to the Azure AI Foundry project and configure Azure Monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_insights_connection_string = project.telemetry.get_connection_string()\n",
    "\n",
    "if not application_insights_connection_string:\n",
    "    print(\"Application Insights was not enabled for this project.\")\n",
    "    print(\"Enable it via the 'Tracing' tab in your AI Foundry project page.\")\n",
    "    exit()\n",
    "    \n",
    "configure_azure_monitor(connection_string=application_insights_connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will define a function that will be used by our agent to get weather information.\n",
    "\n",
    "This code was instrumented using the **trace** class from OpenTelemetry to generate information about its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = os.path.basename(\"Weather Agent\")\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# The tracer.start_as_current_span decorator will trace the function call and enable adding additional attributes\n",
    "# to the span in the function implementation. Note that this will trace the function parameters and their values.\n",
    "@tracer.start_as_current_span(\"fetch_weather\")  # type: ignore\n",
    "def fetch_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the weather information for the specified location.\n",
    "\n",
    "    :param location (str): The location to fetch weather for.\n",
    "    :return: Weather information as a JSON string.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # In a real-world scenario, you'd integrate with a weather API.\n",
    "    # Here, we'll mock the response.\n",
    "    mock_weather_data = {\"New York\": \"Sunny, 25¬∞C\",\n",
    "                         \"London\": \"Cloudy, 18¬∞C\", \"Tokyo\": \"Rainy, 22¬∞C\"}\n",
    "\n",
    "    # Adding attributes to the current span\n",
    "    span = trace.get_current_span()\n",
    "    span.set_attribute(\"requested_location\", location)\n",
    "\n",
    "    weather = mock_weather_data.get(\n",
    "        location, \"Weather data not available for this location.\")\n",
    "    weather_json = json.dumps({\"weather\": weather})\n",
    "    return weather_json\n",
    "\n",
    "\n",
    "# Statically defined user functions for fast reference\n",
    "user_functions: Set[Callable[..., Any]] = {\n",
    "    fetch_weather,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the last piece of the puzzle, let's create an agent, and ask him about the weather in New York.  During its execution telemetry will be collected and pushed to Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize function tool with user function\n",
    "functions = FunctionTool(functions=user_functions)\n",
    "\n",
    "with tracer.start_as_current_span(scenario):    \n",
    "    \n",
    "    agent = project.agents.create_agent(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        name=\"Weather Agent\",\n",
    "        instructions=\"You are a helpful agent which help people find information about the weather.\",\n",
    "        tools=functions.definitions,\n",
    "    )    \n",
    "\n",
    "    thread = project.agents.create_thread()\n",
    "\n",
    "    message = project.agents.create_message(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=\"Hello, what is the weather in New York?\",\n",
    "    )\n",
    "\n",
    "    run = project.agents.create_run(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=agent.id\n",
    "    )\n",
    "    \n",
    "    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "        time.sleep(1)\n",
    "        run = project.agents.get_run(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "        if run.status == \"requires_action\" and isinstance(run.required_action, SubmitToolOutputsAction):\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            if not tool_calls:\n",
    "                print(\"No tool calls provided - cancelling run\")\n",
    "                project.agents.cancel_run(thread_id=thread.id, run_id=run.id)\n",
    "                break\n",
    "\n",
    "            tool_outputs = []\n",
    "            for tool_call in tool_calls:\n",
    "                if isinstance(tool_call, RequiredFunctionToolCall):\n",
    "                    try:\n",
    "                        output = functions.execute(tool_call)\n",
    "                        tool_outputs.append(\n",
    "                            ToolOutput(\n",
    "                                tool_call_id=tool_call.id,\n",
    "                                output=output,\n",
    "                            )\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing tool_call {tool_call.id}: {e}\")\n",
    "\n",
    "            print(f\"Tool outputs: {tool_outputs}\")\n",
    "            if tool_outputs:\n",
    "                project.agents.submit_tool_outputs_to_run(\n",
    "                    thread_id=thread.id, run_id=run.id, tool_outputs=tool_outputs\n",
    "                )\n",
    "\n",
    "            print(f\"Current run status: {run.status}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds go to Azure AI Foundry, select the project where the telemetry was pushed and in the tracing option (highlighted in yellow) you will find the telemetry that was collected and sent.\n",
    "\n",
    "<img src=\"./assets/img/ai-foundry-tracing.png\" width=\"70%\">\n",
    "\n",
    "You have reached the end of this lab. üëè"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
