{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Grounding with Bing Search\n",
    "\n",
    "**Grounding with Bing Search** allows your Azure AI Agents to incorporate real-time public web data when generating responses. You need to create a Grounding with Bing Search resource, and then connect this resource to your Azure AI Agents. When a user sends a query, Azure AI Agents decide if Grounding with Bing Search should be leveraged or not. If so, it will leverage Bing to search over public web data and return relevant chunks. Lastly, Azure AI Agents will use returned chunks to generate a response.  \n",
    "\n",
    "You can ask questions such as \"*what is the top news today*\" or \"*what is the recent update in the retail industry in the US?*\", which require real-time public data.\n",
    "\n",
    "Developers and end users don't have access to raw content returned from Grounding with Bing Search. The response, however, includes citations with links to the websites used to generate the response, and a link to the Bing query used for the search. These two *References* must be retained and displayed in the exact form provided by Microsoft, as per Grounding with Bing Search's [Use and Display Requirements](https://www.microsoft.com/en-us/bing/apis/grounding-legal#use-and-display-requirements). See the [how to display Grounding with Bing Search results](#how-to-display-grounding-with-bing-search-results) section for details.\n",
    "\n",
    ">[!IMPORTANT]\n",
    "> 1. Your usage of Grounding with Bing Search may incur costs. See the [pricing page](https://www.microsoft.com/bing/apis/grounding-pricing) for details.\n",
    "> 1. By creating and using a Grounding with Bing Search resource through code-first experience, such as Azure CLI, or deploying through deployment template, you agree to be bound by and comply with the terms available at https://www.microsoft.com/en-us/bing/apis/grounding-legal, which may be updated from time to time.\n",
    "\n",
    "\n",
    "## **Setup:**  \n",
    "\n",
    "\n",
    "1. Register the Bing Search provider using the command below on Windows Terminal.\n",
    "\n",
    "   ```console\n",
    "       az provider register --namespace 'Microsoft.Bing'\n",
    "   ```\n",
    "\n",
    "1. Create a new Grounding with Bing Search resource in the [Azure portal](https://portal.azure.com/#create/Microsoft.BingGroundingSearch), and select the different fields in the creation form. Make sure you create this Grounding with Bing Search resource in the same resource group as your Azure AI Agent, AI Project, and other resources.\n",
    "\n",
    "1. After you have created a Grounding with Bing Search resource, you can find it in [Azure portal](https://portal.azure.com/#home). Navigate to the resource group you've created the resource in, search for the Grounding with Bing Search resource you have created.\n",
    "\n",
    "   ![alt text](assets/img/resource-azure-portal.png)\n",
    "\n",
    "1. Select the Grounding with Bing Search resource you have created and copy any of the API keys.\n",
    "\n",
    "    ![alt text](assets/img/key-resource-azure-portal.png)\n",
    "\n",
    "1. Go to the [Azure AI Foundry portal](https://ai.azure.com/) and select the AI Project (make sure it's in the same resource group of your Grounding with Bing Search resource). Click **management center**.\n",
    "\n",
    "    ![alt text](assets/img/project-settings-button.png)\n",
    "\n",
    "1. Select **+ new connection** in the settings page. \n",
    "\n",
    "    >[!NOTE]\n",
    "    > If you re-generate the API key at a later date, you need to update the connection with the new key.\n",
    "\n",
    "    ![alt text](assets/img/project-connections.png)\n",
    "\n",
    "1. Select **API key** in **other resource types**.\n",
    "\n",
    "    ![alt text](assets/img/api-key-connection.png)\n",
    "\n",
    "1. Enter the following information and then create a new connection to your Grounding with Bing Search resource.\n",
    "\n",
    "    - Endpoint: `https://api.bing.microsoft.com/`\n",
    "    - Key: `YOUR_API_KEY`\n",
    "    - Connection name: `YOUR_CONNECTION_NAME` (You will use this connection name in the sample code below.)\n",
    "    - Access: you can choose either *this project only* or *shared to all projects*. Just make sure in the sample code below, the project you entered connection string for has access to this connection.\n",
    "\n",
    "1. Deploy a GPT-4o model as defined in the image below. The version must be **2024-08-06** and the token per minute must be at **least 3 million tokens**.\n",
    "\n",
    "\n",
    "<img src=\"./assets/img/create-gpt4o.png\" width=\"50%\">\n",
    "\n",
    "Now that we have configured everything, let's create a client object, which will contain the connection string for connecting to your AI project and other resources. To make the Grounding with Bing search tool available to your agent, use a connection to initialize the tool and attach it to the agent. You can find your connection in the **connected resources** section of your project in the Azure AI Foundry portal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pp as pp\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import BingGroundingTool\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "def pprint(obj):\n",
    "    pp(obj.as_dict() if hasattr(obj, \"as_dict\") else obj, width=100)\n",
    "\n",
    "\n",
    "# Print the environments we will be using.\n",
    "print(f\"PROJECT_CONNECTION_STRING: {os.getenv('PROJECT_CONNECTION_STRING')}\")\n",
    "print(f\"BING_CONNECTION_NAME: {os.getenv('BING_CONNECTION_NAME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bing_connection = project_client.connections.get(\n",
    "    connection_name=os.environ[\"BING_CONNECTION_NAME\"]\n",
    ")\n",
    "conn_id = bing_connection.id\n",
    "\n",
    "print(conn_id)\n",
    "\n",
    "# Initialize agent bing tool and add the connection id\n",
    "bing = BingGroundingTool(connection_id=conn_id)\n",
    "\n",
    "# Create agent with the bing tool and process assistant run\n",
    "project_client.__enter__()\n",
    "agent = project_client.agents.create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    name=\"my-assistant\",\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools=bing.definitions,\n",
    "    headers={\"x-ms-enable-preview\": \"true\"}\n",
    ")\n",
    "print(f\"Created agent, ID: {agent.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create thread for communication\n",
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "# Create message to thread\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What is the top news today\",\n",
    ")\n",
    "print(f\"Created message, ID: {message.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and process agent run in thread with tools\n",
    "run = project_client.agents.create_and_process_run(thread_id=thread.id, assistant_id=agent.id)\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "print(f\"Run ID: {run.id}\")\n",
    "\n",
    "# Retrieve run step details to get Bing Search query link\n",
    "# To render the webpage, we recommend you replace the endpoint of Bing search query URLs with `www.bing.com` and your Bing search query URL would look like \"https://www.bing.com/search?q={search query}\"\n",
    "run_steps = project_client.agents.list_run_steps(run_id=run.id, thread_id=thread.id)\n",
    "run_steps_data = run_steps['data']\n",
    "print(f\"Last run step detail: {run_steps_data}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "\n",
    "messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the agent once done\n",
    "project_client.agents.delete_agent(agent.id)\n",
    "print(\"Deleted agent\")\n",
    "project_client.__exit__(None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file search tool uses the Azure AI Search and Azure Blob Storage resources you connected during agent setup.\n",
    "\n",
    "* Uploaded files get stored in your connected Azure Blob Storage account\n",
    "* Vector stores get created using your connected Azure AI Search resource\n",
    "\n",
    "For both agent setups, Azure OpenAI handles the entire ingestion process, which includes:\n",
    "\n",
    "* Automatically parsing and chunking documents\n",
    "* Generating and storing embeddings\n",
    "* Utilizing both vector and keyword searches to retrieve relevant content for user queries.\n",
    "There is no difference in the code between the two setups; the only variation is in where your files and created vector stores are stored.\n",
    "\n",
    "## How it works\n",
    "The file search tool implements several retrieval best practices out of the box to help you extract the right data from your files and augment the model’s responses. The file search tool:\n",
    "\n",
    "* Rewrites user queries to optimize them for search.\n",
    "* Breaks down complex user queries into multiple searches it can run in parallel.\n",
    "* Runs both keyword and semantic searches across both agent and thread vector stores.\n",
    "* Reranks search results to pick the most relevant ones before generating the final response.\n",
    "* By default, the file search tool uses the following settings:\n",
    "    * Chunk size: 800 tokens\n",
    "    * Chunk overlap: 400 tokens\n",
    "    * Embedding model: text-embedding-3-large at 256 dimensions\n",
    "    * Maximum number of chunks added to context: 20\n",
    "\n",
    "## Vector stores\n",
    "Vector store objects give the file search tool the ability to search your files. Adding a file to a vector store automatically parses, chunks, embeds, and stores the file in a vector database that's capable of both keyword and semantic search. Each vector store can hold up to 10,000 files. Vector stores can be attached to both agents and threads. Currently you can attach at most one vector store to an agent and at most one vector store to a thread.\n",
    "\n",
    "Similarly, these files can be removed from a vector store by either:\n",
    "\n",
    "* Deleting the vector store file object or,\n",
    "* By deleting the underlying file object, which removes the file it from all vector_store and code_interpreter configurations across all agents and threads in your organization\n",
    "The maximum file size is 512 MB. Each file should contain no more than 5,000,000 tokens per file (computed automatically when you attach a file).\n",
    "\n",
    "## Ensuring vector store readiness before creating runs\n",
    "We highly recommend that you ensure all files in a vector_store are fully processed before you create a run. This ensures that all the data in your vector store is searchable. You can check for vector store readiness by using the polling helpers in the SDKs, or by manually polling the vector store object to ensure the status is completed.\n",
    "\n",
    "As a fallback, there's a 60-second maximum wait in the run object when the thread's vector store contains files that are still being processed. This is to ensure that any files your users upload in a thread a fully searchable before the run proceeds. This fallback wait does not apply to the agent's vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Considerations:** \n",
    ">    * According to Grounding with Bing's [terms of use and use and display requirements](https://www.microsoft.com/en-us/bing/apis/grounding-legal#use-and-display-requirements), you need to display both website URLs and Bing search query URLs in your custom interface. You can find website URLs through `annotations` parameter in API response and Bing search query URLs through `runstep` details. To render the webpage, we recommend you replace the endpoint of Bing search query URLs with `www.bing.com` and your Bing search query URL would look like \"https://www.bing.com/search?q={search query}\"\n",
    ">    * Microsoft will use data you send to Grounding with Bing to improve Microsoft products and services. Where you send personal data to this service, you are responsible for obtaining sufficient consent from the data subjects. The Data Protection Terms in the Online Services Terms do not apply to Grounding with Bing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Grounding with file to assistant API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart – Upload Local Files with file search\n",
    "In this example, we use Azure AI Agent Service to create an agent that can help answer questions on information you upload from local files.\n",
    "\n",
    "### Prerequisites\n",
    "Complete the agent setup.\n",
    "\n",
    "* Ensure that you have the role Storage Blob Data Contributor on your project's storage account.\n",
    "* Ensure that you have the role Azure AI Developer on your project.\n",
    "\n",
    "### Create your agent\n",
    "Make sure that once you have created the project client you upload your file and create a vector store using its *file.id*. Once that has been done create your agent and add the file search tool definitions and resources respectively to *tools* and to *tool_resources* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import FileSearchTool, MessageAttachment, FilePurpose\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\n",
    "# Create an Azure AI Client from a connection string, copied from your Azure AI Foundry project.\n",
    "# At the moment, it should be in the format \"<HostName>;<AzureSubscriptionId>;<ResourceGroup>;<ProjectName>\"\n",
    "# Customer needs to login to Azure subscription via Azure CLI and set the environment variables\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=credential, conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"] \n",
    ")\n",
    "# We will upload the local file and will use it for vector store creation.\n",
    "\n",
    "# upload a file\n",
    "file = project_client.agents.upload_file_and_poll(file_path='./assets/data/product_info.md', purpose=FilePurpose.AGENTS)\n",
    "print(f\"Uploaded file, file ID: {file.id}\")\n",
    "\n",
    "# create a vector store with the file you uploaded\n",
    "vector_store = project_client.agents.create_vector_store_and_poll(file_ids=[file.id], name=\"my_vectorstore\")\n",
    "print(f\"Created vector store, vector store ID: {vector_store.id}\")\n",
    "# create a file search tool\n",
    "file_search_tool = FileSearchTool(vector_store_ids=[vector_store.id])\n",
    "\n",
    "# notices that FileSearchTool as tool and tool_resources must be added or the agent will be unable to search the file\n",
    "agent = project_client.agents.create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    name=\"my-agent\",\n",
    "    instructions=\"You are a helpful agent\",\n",
    "    tools=file_search_tool.definitions,\n",
    "    tool_resources=file_search_tool.resources,\n",
    ")\n",
    "print(f\"Created agent, agent ID: {agent.id}\")\n",
    "\n",
    "thread_grounding = project_client.agents.create_thread()\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread_grounding.id,\n",
    "    role=\"user\",\n",
    "    content=\"I have an issue with the tent poles. Are those covered by the waranty?\",\n",
    ")\n",
    "\n",
    " # Run the agent\n",
    "run = project_client.agents.create_and_process_run(thread_id=thread_grounding.id, assistant_id=agent.id)\n",
    "print(f\"Run finished with status: {run.status}\")\n",
    "\n",
    "if run.status == \"failed\":\n",
    "    # Check if you got \"Rate limit is exceeded.\", then you want to get more quota\n",
    "    print(f\"Run failed: {run.last_error}\")\n",
    "\n",
    "# Get messages from the thread\n",
    "messages = project_client.agents.list_messages(thread_id=thread_grounding.id)\n",
    "pprint(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way this can be done is by attaching the files as Message attachments on your thread. Doing so creates another vector_store associated with the thread, or, if there's already a vector store attached to this thread, attaches the new files to the existing thread vector store. When you create a Run on this thread, the file search tool queries both the vector_store from your agent and the vector_store on the thread.\n",
    "\n",
    "Try it for your self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thread\n",
    "thread = project_client.agents.create_thread()\n",
    "print(f\"Created thread, thread ID: {thread.id}\")\n",
    "\n",
    "# Upload the user provided file as a messsage attachment\n",
    "message_file = project_client.agents.upload_file_and_poll(\n",
    "    file_path='./assets/data/product_info.md', purpose=FilePurpose.AGENTS)\n",
    "print(f\"Uploaded file, file ID: {message_file.id}\")\n",
    "\n",
    "# Create a message with the file search attachment\n",
    "# Notice that vector store is created temporarily when using attachments with a default expiration policy of seven days.\n",
    "attachment = MessageAttachment(\n",
    "    file_id=message_file.id, tools=FileSearchTool().definitions)\n",
    "message = project_client.agents.create_message(\n",
    "    thread_id=thread.id, role=\"user\", content=\"What feature does Smart Eyewear offer?\", attachments=[attachment]\n",
    ")\n",
    "print(f\"Created message, message ID: {message.id}\")\n",
    "run = project_client.agents.create_and_process_run(\n",
    "    thread_id=thread.id, assistant_id=agent.id)\n",
    "print(f\"Created run, run ID: {run.id}\")\n",
    "\n",
    "# Get messages from the thread\n",
    "messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "pprint(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
